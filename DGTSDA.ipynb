{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a52232eb-b25b-4ca4-806f-91c68e9e8b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/lu/lib/python3.10/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /root/anaconda3/envs/lu/lib/python3.10/site-packages/libpyg.so: undefined symbol: _ZN2at23SavedTensorDefaultHooks11set_tracingEb\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/root/anaconda3/envs/lu/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /root/anaconda3/envs/lu/lib/python3.10/site-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZN2at23SavedTensorDefaultHooks11set_tracingEb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/root/anaconda3/envs/lu/lib/python3.10/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /root/anaconda3/envs/lu/lib/python3.10/site-packages/torch_spline_conv/_basis_cuda.so: undefined symbol: _ZN2at23SavedTensorDefaultHooks11set_tracingEb\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/lu/lib/python3.10/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /root/anaconda3/envs/lu/lib/python3.10/site-packages/torch_sparse/_spmm_cuda.so: undefined symbol: _ZN2at23SavedTensorDefaultHooks11set_tracingEb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "#from torch_geometric.nn.dense.linear import Linear\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv, APPNP, GCN2Conv, GATv2Conv, ResGatedGraphConv, GENConv\n",
    "from typing import Optional\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.typing import (\n",
    "    Adj,\n",
    "    OptPairTensor,\n",
    "    OptTensor,\n",
    "    SparseTensor,\n",
    ")\n",
    "from  torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf4230f-65a3-40b2-bb93-ef417c7255d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph_Conv(MessagePassing):\n",
    "\n",
    "    _cached_edge_index: Optional[OptPairTensor]\n",
    "    _cached_adj_t: Optional[SparseTensor]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        improved: bool = False,\n",
    "        cached: bool = False,\n",
    "        add_self_loops: Optional[bool] = None,\n",
    "        normalize: bool = True,\n",
    "        bias: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if add_self_loops is None:\n",
    "            add_self_loops = normalize\n",
    "\n",
    "        if add_self_loops and not normalize:\n",
    "            raise ValueError(f\"'{self.__class__.__name__}' does not support \"\n",
    "                             f\"adding self-loops to the graph when no \"\n",
    "                             f\"on-the-fly normalization is applied\")\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self._cached_edge_index = None\n",
    "        self._cached_adj_t = None\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        self._cached_edge_index = None\n",
    "        self._cached_adj_t = None\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj,\n",
    "                edge_weight: OptTensor = None) -> Tensor:\n",
    "\n",
    "        if self.normalize:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                cache = self._cached_edge_index\n",
    "                if cache is None:\n",
    "                    edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
    "                        edge_index, edge_weight, x.size(self.node_dim))\n",
    "                    if self.cached:\n",
    "                        self._cached_edge_index = (edge_index, edge_weight)\n",
    "                else:\n",
    "                    edge_index, edge_weight = cache[0], cache[1]\n",
    "\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                cache = self._cached_adj_t\n",
    "                if cache is None:\n",
    "                    edge_index = gcn_norm(  # yapf: disable\n",
    "                        edge_index, edge_weight, x.size(self.node_dim))\n",
    "                    if self.cached:\n",
    "                        self._cached_adj_t = edge_index\n",
    "                else:\n",
    "                    edge_index = cache\n",
    "\n",
    "        out = self.propagate(edge_index, x=x, edge_weight=edge_weight)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_weight: OptTensor) -> Tensor:\n",
    "        return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: Adj, x: Tensor) -> Tensor:\n",
    "        return spmm(adj_t, x, reduce=self.aggr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7757e0ac-2cc8-4944-a27c-412528b599a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_attention_conv(q, k, v, output_attn=False):\n",
    "    sqrt_n = torch.sqrt(torch.tensor(q.shape[0], dtype=torch.float32))\n",
    "\n",
    "    a = torch.einsum(\"lmh,ldh->mdh\", k/sqrt_n, v)\n",
    "\n",
    "    attention = torch.softmax(a, dim=0)\n",
    "\n",
    "    output = torch.einsum(\"lmh,mdh->ldh\", q, attention)\n",
    "\n",
    "    if output_attn:\n",
    "        return output, attention\n",
    "\n",
    "    return output\n",
    "\n",
    "class TransConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels,\n",
    "                 out_channels,\n",
    "                 num_heads,\n",
    "                 use_weight=True):\n",
    "        super().__init__()\n",
    "        self.Wk = nn.Linear(in_channels, out_channels * num_heads)\n",
    "        self.Wq = nn.Linear(in_channels, out_channels * num_heads)\n",
    "        if use_weight:\n",
    "            self.Wv = nn.Linear(in_channels, out_channels * num_heads)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.num_heads = num_heads\n",
    "        self.use_weight = use_weight\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.Wk.reset_parameters()\n",
    "        self.Wq.reset_parameters()\n",
    "        if self.use_weight:\n",
    "            self.Wv.reset_parameters()\n",
    "\n",
    "    def forward(self, query_input, source_input, output_attn=False):\n",
    "        # feature transformation\n",
    "        query = self.Wq(query_input).reshape(-1, self.out_channels ,\n",
    "                                             self.num_heads)\n",
    "        key = self.Wk(source_input).reshape(-1, self.out_channels ,\n",
    "                                            self.num_heads)\n",
    "        if self.use_weight:\n",
    "            value = self.Wv(source_input).reshape(-1, self.out_channels,\n",
    "                                                  self.num_heads)\n",
    "        else:\n",
    "            value = source_input.reshape(-1, self.out_channels, 1)\n",
    "\n",
    "        # compute full attentive aggregation\n",
    "        if output_attn:\n",
    "            attention_output, attn = full_attention_conv(\n",
    "                query, key, value, output_attn)  # [N, H, D]\n",
    "        else:\n",
    "            attention_output = full_attention_conv(\n",
    "                query, key, value)  # [N, H, D]\n",
    "\n",
    "        final_output = attention_output\n",
    "        final_output = final_output.mean(dim=-1)\n",
    "\n",
    "        if output_attn:\n",
    "            return final_output, attn\n",
    "        else:\n",
    "            return final_output\n",
    "\n",
    "class TransConv(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, activation, num_layers=1, num_heads=2,\n",
    "                 alpha=0.1, dropout=0.3, use_bn=True, use_residual=True, use_weight=True, use_act=True):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.fcs = nn.ModuleList()\n",
    "        self.fcs.append(nn.Linear(in_channels, hidden_channels))\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(\n",
    "                TransConvLayer(hidden_channels, hidden_channels, num_heads=num_heads, use_weight=use_weight))\n",
    "            self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn\n",
    "        self.residual = use_residual\n",
    "        self.alpha = alpha\n",
    "        self.use_act = use_act\n",
    "        self.activation = activation\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "        for fc in self.fcs:\n",
    "            fc.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        layer_ = []\n",
    "        # input MLP layer\n",
    "        x = self.fcs[0](x)\n",
    "        if self.use_bn:\n",
    "            x = self.bns[0](x)\n",
    "        x = self.activation(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        layer_.append(x)\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, x)\n",
    "            if self.residual:\n",
    "                x = self.alpha * x + (1 - self.alpha) * layer_[i]\n",
    "            if self.use_bn:\n",
    "                x = self.bns[i + 1](x)\n",
    "            if self.use_act:\n",
    "                x = self.activation(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            layer_.append(x)\n",
    "        return x\n",
    "\n",
    "    def get_attentions(self, x):\n",
    "        layer_, attentions = [], []\n",
    "        x = self.fcs[0](x)\n",
    "        if self.use_bn:\n",
    "            x = self.bns[0](x)\n",
    "        x = self.activation(x)\n",
    "        layer_.append(x)\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x, attn = conv(x, x, output_attn=True)\n",
    "            attentions.append(attn)\n",
    "            if self.residual:\n",
    "                x = self.alpha * x + (1 - self.alpha) * layer_[i]\n",
    "            if self.use_bn:\n",
    "                x = self.bns[i + 1](x)\n",
    "            layer_.append(x)\n",
    "        return torch.stack(attentions, dim=0)  # [layer num, N, N]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d09331e9-d8bc-40a0-864a-c11b329ba7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DUALFormer_Model(torch.nn.Module):\n",
    "    def __init__(self, sno_feature, dis_feature, input_dim,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 activation,\n",
    "                 num_gnns,\n",
    "                 num_trans,\n",
    "                 num_heads,\n",
    "                 gat_heads,\n",
    "                 dropout_trans,\n",
    "                 dropout,\n",
    "                 alpha,\n",
    "                 use_bn,\n",
    "                 lammda=0.1,\n",
    "                 GraphConv='sgc'):\n",
    "        super(DUALFormer_Model, self).__init__()\n",
    "\n",
    "        self.sno_feature = sno_feature\n",
    "        self.dis_feature = dis_feature\n",
    "        self.lin_sf = Linear(sno_feature.shape[1], input_dim)\n",
    "        self.lin_df = Linear(dis_feature.shape[1], input_dim)   \n",
    "        \n",
    "        self.activation = activation()\n",
    "        self.num_gnns = num_gnns\n",
    "        self.layers_trans = TransConv(input_dim, hidden_dim, self.activation,\n",
    "                                      num_layers=num_trans, num_heads=num_heads,\n",
    "                                      alpha=alpha, dropout=dropout_trans,\n",
    "                                      use_bn=use_bn, use_residual=True,\n",
    "                                      use_weight=True, use_act=True)\n",
    "\n",
    "        if GraphConv == 'sgc':\n",
    "            self.convs = torch.nn.ModuleList()\n",
    "            for _ in range(num_gnns):\n",
    "                self.convs.append(Graph_Conv())\n",
    "        elif GraphConv == 'gcn':\n",
    "            self.convs = torch.nn.ModuleList()\n",
    "            for _ in range(num_gnns):\n",
    "                self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        elif GraphConv == 'appnp':\n",
    "            self.convs = APPNP(num_gnns, lammda)\n",
    "        elif GraphConv == 'gcn2':\n",
    "            self.convs = GCN2Conv(hidden_dim, lammda, theta = 0.1, layer = num_gnns)\n",
    "        elif GraphConv == 'gat':\n",
    "            self.convs = torch.nn.ModuleList()\n",
    "            for _ in range(num_gnns):\n",
    "                self.convs.append(GATv2Conv(hidden_dim, hidden_dim, heads = gat_heads, concat = False))\n",
    "        elif GraphConv == 'resgatedgcn':\n",
    "            self.convs = torch.nn.ModuleList()\n",
    "            for _ in range(num_gnns):\n",
    "                self.convs.append(ResGatedGraphConv(hidden_dim, hidden_dim))\n",
    "        elif GraphConv == 'gen':\n",
    "            self.convs = torch.nn.ModuleList()\n",
    "            for _ in range(num_gnns):\n",
    "                self.convs.append(GENConv(hidden_dim, hidden_dim))\n",
    "\n",
    "        self.GraphConv = GraphConv\n",
    "        #self.linear_project = Linear(hidden_dim, output_dim, weight_initializer = \"glorot\")\n",
    "        self.linear_project = Linear(hidden_dim, output_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.params1 = list(self.layers_trans.parameters())\n",
    "        self.params2 = list(self.linear_project.parameters())\n",
    "        self.params3 = list(self.lin_sf.parameters())\n",
    "        self.params4 = list(self.lin_df.parameters())\n",
    "\n",
    "        self.traning = True\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        self.layers_trans.reset_parameters()\n",
    "        self.linear_project.reset_parameters()\n",
    "        self.lin_sf.reset_parameters()\n",
    "        self.lin_df.reset_parameters()\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "\n",
    "        sno_x = self.lin_sf(self.sno_feature)\n",
    "        dis_x = self.lin_df(self.dis_feature)\n",
    "        x = torch.cat((sno_x, dis_x), dim = 0)\n",
    "\n",
    "        #x = F.dropout(x, p = self.dropout, training=self.training)\n",
    "        #x = F.relu(x)\n",
    "        z = self.layers_trans(x)\n",
    "        temp = z\n",
    "        if self.GraphConv in ['sgc', 'gcn', 'resgatedgcn', 'gen']:#sgc, gcn\n",
    "            for i, conv in enumerate(self.convs):\n",
    "                #z = F.dropout(z, p = self.dropout, training=self.training)\n",
    "                #z = F.relu(z)\n",
    "                z = conv(z, edge_index)\n",
    "        elif self.GraphConv == 'appnp':\n",
    "            z = self.convs(z, edge_index) #appnp\n",
    "        elif self.GraphConv == 'gcn2':\n",
    "            z = self.convs(z, temp, edge_index)\n",
    "        elif self.GraphConv == 'gat':\n",
    "            for i, conv in enumerate(self.convs):\n",
    "                z = conv(z, edge_index)\n",
    "                \n",
    "        z = F.dropout(z, p=self.dropout, training=self.training)\n",
    "        z = self.linear_project(z)\n",
    "        adj_rec = torch.sigmoid(torch.mm(z, z.T))\n",
    "\n",
    "        return adj_rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2684e481-2a7e-4817-89fd-1dda669b8b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_adj_mat(training_mask):\n",
    "    adj_tmp = training_mask.copy()\n",
    "    rna_mat = np.zeros((training_mask.shape[0], training_mask.shape[0]))\n",
    "    dis_mat = np.zeros((training_mask.shape[1], training_mask.shape[1]))\n",
    "\n",
    "    mat1 = np.hstack((rna_mat, adj_tmp))\n",
    "    mat2 = np.hstack((adj_tmp.T, dis_mat))\n",
    "    ret = np.vstack((mat1, mat2))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0398166a-3650-45b3-9551-7911011c44ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(real_score, predict_score, roc_path, pr_path, i):\n",
    "    real_score, predict_score = real_score.flatten(), predict_score.flatten()\n",
    "    sorted_predict_score = np.array(\n",
    "        sorted(list(set(np.array(predict_score).flatten()))))\n",
    "    sorted_predict_score_num = len(sorted_predict_score)\n",
    "    thresholds = sorted_predict_score[np.int32(\n",
    "        sorted_predict_score_num*np.arange(1, 1000)/1000)]\n",
    "    thresholds = np.mat(thresholds)\n",
    "    thresholds_num = thresholds.shape[1]\n",
    "\n",
    "    predict_score_matrix = np.tile(predict_score, (thresholds_num, 1))\n",
    "    negative_index = np.where(predict_score_matrix < thresholds.T)\n",
    "    positive_index = np.where(predict_score_matrix >= thresholds.T)\n",
    "    predict_score_matrix[negative_index] = 0\n",
    "    predict_score_matrix[positive_index] = 1\n",
    "    TP = predict_score_matrix.dot(real_score.T)\n",
    "    FP = predict_score_matrix.sum(axis=1)-TP\n",
    "    FN = real_score.sum()-TP\n",
    "    TN = len(real_score.T)-TP-FP-FN\n",
    "\n",
    "    fpr = FP/(FP+TN)\n",
    "    tpr = TP/(TP+FN)\n",
    "    ROC_dot_matrix = np.mat(sorted(np.column_stack((fpr, tpr)).tolist())).T\n",
    "    ROC_dot_matrix.T[0] = [0, 0]\n",
    "    ROC_dot_matrix = np.c_[ROC_dot_matrix, [1, 1]]\n",
    "\n",
    "    #np.savetxt(roc_path.format(i), ROC_dot_matrix)\n",
    "\n",
    "    x_ROC = ROC_dot_matrix[0].T\n",
    "    y_ROC = ROC_dot_matrix[1].T\n",
    "    auc = 0.5*(x_ROC[1:]-x_ROC[:-1]).T*(y_ROC[:-1]+y_ROC[1:])\n",
    "\n",
    "    recall_list = tpr\n",
    "    precision_list = TP/(TP+FP)\n",
    "    PR_dot_matrix = np.mat(sorted(np.column_stack(\n",
    "        (recall_list, precision_list)).tolist())).T\n",
    "    PR_dot_matrix.T[0] = [0, 1]\n",
    "    PR_dot_matrix = np.c_[PR_dot_matrix, [1, 0]]\n",
    "\n",
    "    #np.savetxt(pr_path.format(i), PR_dot_matrix)\n",
    "\n",
    "    x_PR = PR_dot_matrix[0].T\n",
    "    y_PR = PR_dot_matrix[1].T\n",
    "    aupr = 0.5*(x_PR[1:]-x_PR[:-1]).T*(y_PR[:-1]+y_PR[1:])\n",
    "\n",
    "    f1_score_list = 2*TP/(len(real_score.T)+TP-TN)\n",
    "    accuracy_list = (TP+TN)/len(real_score.T)\n",
    "    specificity_list = TN/(TN+FP)\n",
    "    # plt.plot(x_ROC, y_ROC)\n",
    "    # plt.plot(x_PR,y_PR)\n",
    "    # plt.show()\n",
    "    max_index = np.argmax(f1_score_list)\n",
    "    f1_score = f1_score_list[max_index]\n",
    "    accuracy = accuracy_list[max_index]\n",
    "    specificity = specificity_list[max_index]\n",
    "    recall = recall_list[max_index]\n",
    "    precision = precision_list[max_index]\n",
    "    print( ' auc:{:.4f} ,aupr:{:.4f},f1_score:{:.4f}, accuracy:{:.4f}, recall:{:.4f}, specificity:{:.4f}, precision:{:.4f}'.format( auc[0, 0],aupr[0, 0], f1_score, accuracy, recall, specificity, precision))\n",
    "    return [auc[0, 0], aupr[0, 0], f1_score, accuracy, recall, specificity, precision]\n",
    "\n",
    "\n",
    "def cv_model_evaluate(interaction_matrix, predict_matrix, train_matrix):\n",
    "    test_index = np.where(train_matrix == 0)\n",
    "    real_score = interaction_matrix[test_index]\n",
    "    predict_score = predict_matrix[test_index]\n",
    "    return get_metrics(real_score, predict_score)\n",
    "\n",
    "\n",
    "# turn dense matrix into a sparse foramt\n",
    "def dense2sparse(matrix: np.ndarray):\n",
    "    mat_coo = coo_matrix(matrix)\n",
    "    edge_idx = np.vstack((mat_coo.row, mat_coo.col))\n",
    "    return edge_idx, mat_coo.data\n",
    "\n",
    "def calculate_loss(pred, pos_edge_idx, neg_edge_idx, device):\n",
    "    pos_pred_socres = pred[pos_edge_idx[0], pos_edge_idx[1]]\n",
    "    neg_pred_socres = pred[neg_edge_idx[0], neg_edge_idx[1]]\n",
    "    pred_scores = torch.hstack((pos_pred_socres, neg_pred_socres))\n",
    "    true_labels = torch.hstack((torch.ones(pos_pred_socres.shape[0]), torch.zeros(neg_pred_socres.shape[0]))).to(device)\n",
    "    loss_fun=torch.nn.BCELoss(reduction='mean')\n",
    "    # loss_fun=torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    return loss_fun(pred_scores, true_labels)\n",
    "\n",
    "def calculate_evaluation_metrics(pred_mat, pos_edges, neg_edges, roc_path, pr_path, i):\n",
    "    pos_pred_socres = pred_mat[pos_edges[0], pos_edges[1]]\n",
    "    neg_pred_socres = pred_mat[neg_edges[0], neg_edges[1]]\n",
    "    pred_labels = np.hstack((pos_pred_socres, neg_pred_socres))\n",
    "    true_labels = np.hstack((np.ones(pos_pred_socres.shape[0]), np.zeros(neg_pred_socres.shape[0])))\n",
    "    return get_metrics(true_labels, pred_labels, roc_path, pr_path, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "412facba-fbb4-4044-b40d-e5be279f22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 10086\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a3e6880-4864-4860-8548-60d978d14507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3357534/749980767.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_all = torch.load(input_graph_all)\n",
      "/tmp/ipykernel_3357534/749980767.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  snorna_feature = torch.tensor(graph_all[\"snorna\"][\"x\"], dtype=torch.float, device = device)\n",
      "/tmp/ipykernel_3357534/749980767.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  disease_feature = torch.tensor(graph_all[\"disease\"][\"x\"], dtype=torch.float, device = device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_graph_all = r\"SDI/data/graph_data/hetero_graph_2-4.pkl\"\n",
    "graph_all = torch.load(input_graph_all)\n",
    "\n",
    "snorna_feature = torch.tensor(graph_all[\"snorna\"][\"x\"], dtype=torch.float, device = device)\n",
    "disease_feature = torch.tensor(graph_all[\"disease\"][\"x\"], dtype=torch.float, device = device)\n",
    "snorna_disease_edge_index = graph_all[\"snorna\", \"to\", \"disease\"][\"edge_index\"]\n",
    "snorna_disease_mat = np.zeros((snorna_feature.shape[0], disease_feature.shape[0]))\n",
    "snorna_disease_mat[snorna_disease_edge_index[0], snorna_disease_edge_index[1]] = 1\n",
    "\n",
    "new_adj_mat = construct_adj_mat(snorna_disease_mat)\n",
    "edge_index =  np.array(tuple(np.where(new_adj_mat !=0)))\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6257b88-78cd-4a26-9c1e-046ecb1e7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(edge_index, new_adj_mat, snorna_feature, disease_feature, args_config, device):\n",
    "    # initialize parameters\n",
    "    lr = args_config['lr']\n",
    "    weight_decay = args_config['weight_decay']\n",
    "    kfolds = args_config['kfolds']\n",
    "    num_epoch = args_config['num_epoch']    \n",
    "    input_dim = args_config['input_dim']\n",
    "    activation = args_config['activation']\n",
    "    num_gnns = args_config['num_gnns']\n",
    "    num_heads = args_config['num_heads']\n",
    "    gat_heads = args_config['gat_heads']\n",
    "    num_trans = args_config['num_trans']\n",
    "    dropout_trans = args_config['dropout_trans']\n",
    "    dropout = args_config['dropout']\n",
    "    alpha = args_config['alpha']\n",
    "    use_bn = args_config['use_bn']\n",
    "    lammda = args_config['lammda']\n",
    "    GraphConv = args_config['GraphConv']\n",
    "    hidden_dim = args_config['hidden_dim']\n",
    "    output_dim = args_config['output_dim']      \n",
    "\n",
    "    rng = np.random.default_rng(10086)\n",
    "    pos_samples, edge_attr = dense2sparse(new_adj_mat)\n",
    "    pos_samples_shuffled = rng.permutation(pos_samples, axis=1)\n",
    "\n",
    "    # get the edge index of negative samples\n",
    "    rng = np.random.default_rng(10086)\n",
    "    neg_samples = np.where(new_adj_mat == 0)\n",
    "    neg_samples_shuffled = rng.permutation(neg_samples, axis=1)[:, :pos_samples_shuffled.shape[1]]\n",
    "    \n",
    "    pos_edges = pos_samples_shuffled\n",
    "    neg_edges = neg_samples_shuffled\n",
    "    idx = np.arange(pos_edges.shape[1])\n",
    "    np.random.shuffle(idx)\n",
    "    idx_splited = np.array_split(idx, kfolds)\n",
    "    metrics_tensor = np.zeros((1, 7))\n",
    "    for i in range(kfolds):\n",
    "        tmp = []\n",
    "        for j in range(1, kfolds):\n",
    "            tmp.append(idx_splited[(j + i) % kfolds])\n",
    "        tmp = np.concatenate(tmp)\n",
    "        training_pos_edges = pos_edges[:, tmp]\n",
    "        training_neg_edges = neg_edges[:, tmp]\n",
    "        test_pos_edges = pos_edges[:, idx_splited[i]]\n",
    "        test_neg_edges = neg_edges[:, idx_splited[i]]      \n",
    "\n",
    "        print(f'################Fold {i + 1} of {kfolds}################')\n",
    "        #sno_feature, dis_feature, hidden_dim = 64, out_dim = 32, K = [16, 8, 4], drop_rate = 0.5\n",
    "        model = DUALFormer_Model(sno_feature = snorna_feature, dis_feature = disease_feature, input_dim = input_dim,\n",
    "                    hidden_dim = hidden_dim,\n",
    "                    output_dim = output_dim,\n",
    "                    activation =  activation,\n",
    "                    num_gnns = num_gnns,\n",
    "                    num_trans = num_trans,\n",
    "                    num_heads = num_heads,\n",
    "                    gat_heads = gat_heads,\n",
    "                    dropout_trans = dropout_trans,\n",
    "                    dropout = dropout,\n",
    "                    alpha = alpha,\n",
    "                    use_bn = use_bn,\n",
    "                    lammda = lammda,\n",
    "                    GraphConv = GraphConv\n",
    "                    ).to(device)\n",
    "        #print(model)\n",
    "\n",
    "        optimizer = optim.Adamax(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        #optimizer = optim.RMSprop(model.parameters(),lr=lr,alpha=0.99, eps=1e-08, momentum=0.1,weight_decay=weight_decay,centered=False)\n",
    "        \"\"\"\n",
    "        base_lr=5e-5       \n",
    "        scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr=lr, step_size_up=200,\n",
    "                                                step_size_down=200, mode='exp_range', gamma=0.99, scale_fn=None,\n",
    "                                                cycle_momentum=False, last_epoch=-1)\n",
    "        \"\"\"\n",
    "        \n",
    "        for epoch in range(num_epoch):\n",
    "            model.train()\n",
    "            output = model(edge_index)\n",
    "            \n",
    "            #print(de_result)\n",
    "            loss = calculate_loss(output, training_pos_edges, training_neg_edges, device = device)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "            \n",
    "            if (epoch + 1) % 500 == 0 or epoch == 0:\n",
    "                pass\n",
    "                print('------EPOCH {} of {}------'.format(epoch + 1, args_config['num_epoch']))\n",
    "                print('Loss: {}'.format(loss))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(edge_index)\n",
    "            \n",
    "            roc_path = r'SDI/data/graph_data/SDI_DUAL_ROC_fold_{}.csv'\n",
    "            pr_path = r'SDI/data/graph_data/SDI_DUAL_PR_fold_{}.csv'\n",
    "\n",
    "            metrics = calculate_evaluation_metrics(output.detach().cpu(), test_pos_edges, \n",
    "                                                   test_neg_edges, roc_path, pr_path, i)\n",
    "            metrics_tensor += metrics\n",
    "            \n",
    "    print('Average result:' ,end='')\n",
    "    avg_metrics = metrics_tensor / kfolds\n",
    "    del metrics_tensor\n",
    "    # print( ' {:.4f} {:.4f} {:.4f}  {:.4f} {:.4f} {:.4f} {:.4f}'\n",
    "    #        .format(avg_metrics[0][0],avg_metrics[0][1],avg_metrics[0][2],avg_metrics[0][3],avg_metrics[0][4],avg_metrics[0][5],avg_metrics[0][6]))\n",
    "    print(avg_metrics)\n",
    "    return avg_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41695418-bb4d-4d34-a0b2-75ac39d72729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################Fold 1 of 10################\n",
      "------EPOCH 1 of 1200------\n",
      "Loss: 2.0123496055603027\n",
      "------EPOCH 500 of 1200------\n",
      "Loss: 0.4511253833770752\n",
      "------EPOCH 1000 of 1200------\n",
      "Loss: 0.4037947356700897\n",
      " auc:0.9847 ,aupr:0.9818,f1_score:0.9558, accuracy:0.9560, recall:0.9505, specificity:0.9615, precision:0.9611\n",
      "################Fold 2 of 10################\n",
      "------EPOCH 1 of 1200------\n",
      "Loss: 3.035507917404175\n",
      "------EPOCH 500 of 1200------\n",
      "Loss: 0.4818693697452545\n",
      "------EPOCH 1000 of 1200------\n",
      "Loss: 0.4142950773239136\n",
      " auc:0.9821 ,aupr:0.9773,f1_score:0.9539, accuracy:0.9533, recall:0.9670, specificity:0.9396, precision:0.9412\n",
      "################Fold 3 of 10################\n",
      "------EPOCH 1 of 1200------\n",
      "Loss: 2.7753405570983887\n",
      "------EPOCH 500 of 1200------\n",
      "Loss: 0.47443529963493347\n",
      "------EPOCH 1000 of 1200------\n",
      "Loss: 0.4202680289745331\n",
      " auc:0.9933 ,aupr:0.9913,f1_score:0.9672, accuracy:0.9670, recall:0.9725, specificity:0.9615, precision:0.9620\n",
      "################Fold 4 of 10################\n",
      "------EPOCH 1 of 1200------\n",
      "Loss: 3.415437936782837\n",
      "------EPOCH 500 of 1200------\n",
      "Loss: 0.4553273022174835\n",
      "------EPOCH 1000 of 1200------\n",
      "Loss: 0.4171161353588104\n",
      " auc:0.9917 ,aupr:0.9886,f1_score:0.9674, accuracy:0.9670, recall:0.9780, specificity:0.9560, precision:0.9570\n",
      "################Fold 5 of 10################\n",
      "------EPOCH 1 of 1200------\n",
      "Loss: 3.8223795890808105\n",
      "------EPOCH 500 of 1200------\n",
      "Loss: 0.46221649646759033\n",
      "------EPOCH 1000 of 1200------\n",
      "Loss: 0.4286063313484192\n",
      " auc:0.9844 ,aupr:0.9843,f1_score:0.9560, accuracy:0.9560, recall:0.9560, specificity:0.9560, precision:0.9560\n",
      "################Fold 6 of 10################\n",
      "------EPOCH 1 of 1200------\n",
      "Loss: 2.8382697105407715\n",
      "------EPOCH 500 of 1200------\n",
      "Loss: 0.444227397441864\n",
      "------EPOCH 1000 of 1200------\n",
      "Loss: 0.4147597551345825\n",
      " auc:0.9807 ,aupr:0.9593,f1_score:0.9596, accuracy:0.9588, recall:0.9780, specificity:0.9396, precision:0.9418\n",
      "################Fold 7 of 10################\n",
      "------EPOCH 1 of 1200------\n",
      "Loss: 2.4191246032714844\n",
      "------EPOCH 500 of 1200------\n",
      "Loss: 0.4618584215641022\n",
      "------EPOCH 1000 of 1200------\n",
      "Loss: 0.4232552647590637\n",
      " auc:0.9878 ,aupr:0.9856,f1_score:0.9563, accuracy:0.9560, recall:0.9615, specificity:0.9505, precision:0.9511\n",
      "################Fold 8 of 10################\n",
      "------EPOCH 1 of 1200------\n",
      "Loss: 2.7871153354644775\n",
      "------EPOCH 500 of 1200------\n",
      "Loss: 0.46368828415870667\n",
      "------EPOCH 1000 of 1200------\n",
      "Loss: 0.41371455788612366\n",
      " auc:0.9789 ,aupr:0.9671,f1_score:0.9514, accuracy:0.9505, recall:0.9670, specificity:0.9341, precision:0.9362\n",
      "################Fold 9 of 10################\n",
      "------EPOCH 1 of 1200------\n",
      "Loss: 1.4223778247833252\n",
      "------EPOCH 500 of 1200------\n",
      "Loss: 0.4499201476573944\n",
      "------EPOCH 1000 of 1200------\n",
      "Loss: 0.4385607838630676\n",
      " auc:0.9853 ,aupr:0.9683,f1_score:0.9650, accuracy:0.9643, recall:0.9835, specificity:0.9451, precision:0.9471\n",
      "################Fold 10 of 10################\n",
      "------EPOCH 1 of 1200------\n",
      "Loss: 3.095179796218872\n",
      "------EPOCH 500 of 1200------\n",
      "Loss: 0.45352673530578613\n",
      "------EPOCH 1000 of 1200------\n",
      "Loss: 0.4272388815879822\n",
      " auc:0.9889 ,aupr:0.9867,f1_score:0.9516, accuracy:0.9505, recall:0.9725, specificity:0.9286, precision:0.9316\n",
      "Average result:[[0.9857641  0.97902331 0.95841557 0.95796703 0.96868132 0.94725275\n",
      "  0.94850023]]\n"
     ]
    }
   ],
   "source": [
    "args_config = {\n",
    "        'kfolds': 10,         \n",
    "        'input_dim' : 256,\n",
    "        'hidden_dim' : 128,\n",
    "        'output_dim' : 64,\n",
    "        'activation' : nn.ReLU,\n",
    "        'num_gnns' : 3,#3 best\n",
    "        'num_trans' : 4,#1-4 up, 5 down\n",
    "        'num_heads' : 4,#\n",
    "        'gat_heads' : 4,#this para work only when uses gat  \n",
    "        'dropout_trans' : 0.2,\n",
    "        'dropout' : 0.2,\n",
    "        'alpha' : 0.1,\n",
    "        'use_bn' : True,\n",
    "        'lammda' : 0.3,\n",
    "        'GraphConv' : \"gcn\",  #\"sgc\"\\\"gcn\"\\\"appnp\"\\\"gcn2\"\\\"gat\"\\\"resgatedgcn\"      \n",
    "        'lr': 0.0005,#1e-3，la是1e-3   #0.0001,0.0005,0.001,0.005,0.01,0.05 \n",
    "        'weight_decay': 0.0001,#5e-3   #0.0001,0.0005,0.001,0.005,0.01,0.05    \n",
    "        'num_epoch' : 1200\n",
    "    }\n",
    "\n",
    "reulst = main(edge_index, new_adj_mat, snorna_feature, disease_feature, args_config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785cce56-25dd-4b16-9184-0612fe5a8df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aff095a-0a93-4cba-afaf-e1aaad4aaf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DUALFormer_Model(sno_feature = snorna_feature, dis_feature = disease_feature, input_dim = 128,\n",
    "                    hidden_dim = 64,\n",
    "                    output_dim = 32,\n",
    "                    activation =  nn.ReLU,\n",
    "                    num_gnns = 2,\n",
    "                    num_trans = 1,\n",
    "                    num_heads = 2,\n",
    "                    gat_heads = 2,\n",
    "                    dropout_trans = 0.5,\n",
    "                    dropout = 0.5,\n",
    "                    alpha = 0.1,\n",
    "                    use_bn = True,\n",
    "                    lammda = 0.1,\n",
    "                    GraphConv = \"gcn\"\n",
    "                    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75b66826-8c1a-4aad-a816-24e6d768b41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch_geometric.typing\n",
    "torch_geometric.typing.WITH_TORCH_SPLINE_CONV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69f4397c-bf31-4011-afc2-90dc5947d4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7aa514b66b90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c325e8b-06d7-4a0d-8959-8c0674c5504c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lu",
   "language": "python",
   "name": "lu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
